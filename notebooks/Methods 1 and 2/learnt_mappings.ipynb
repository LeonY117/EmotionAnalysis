{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from functions import net, MyDataset, train\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  7\n",
      "Number of classes:  6\n"
     ]
    }
   ],
   "source": [
    "eb = pd.read_csv(\"../../data/clean/emobank_affectivetext_lex.csv\")\n",
    "\n",
    "# Ekman classes\n",
    "classes = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "features = [\"VADER_neg\", \"VADER_neu\", \"VADER_pos\", \"VADER_compound\"\\\n",
    "    , \"TextBlob_polarity\", \"TextBlob_subjectivity\", \"AFINN\"]\n",
    "print(\"Number of features: \", len(features))\n",
    "print(\"Number of classes: \", len(classes))\n",
    "\n",
    "labels_map = {\n",
    "    0: \"anger\",\n",
    "    1: \"disgust\",\n",
    "    2: \"fear\",\n",
    "    3: \"joy\",\n",
    "    4: \"sadness\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "# Split into train, test, dev\n",
    "eb_train = eb[eb[\"split\"] == \"train\"]\n",
    "eb_test = eb[eb[\"split\"] == \"test\"]\n",
    "eb_dev = eb[eb[\"split\"] == \"dev\"]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MyDataset(eb_train[features+classes])\n",
    "test_dataset = MyDataset(eb_test[features+classes])\n",
    "dev_dataset = MyDataset(eb_dev[features+classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([920, 7])\n",
      "Labels batch shape: torch.Size([920, 6])\n",
      "tensor([[44.,  8., 78.,  0., 38.,  8.],\n",
      "        [38., 17., 33.,  0., 14.,  7.],\n",
      "        [ 0., 14.,  0., 21.,  0., 58.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  5., 59.,  0., 15.],\n",
      "        [ 0.,  0.,  0., 39.,  0.,  7.],\n",
      "        [ 0.,  0.,  0., 80.,  2., 24.]])\n"
     ]
    }
   ],
   "source": [
    "model = net(len(features), 100, len(classes), num_hidden=3)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1000, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Documents\\GitHub\\EmotionAnalysis\\notebooks\\Methods 1 and 2\\functions.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 148.0979, Valid Loss: 156.4469\n",
      "Epoch: 20, Train Loss: 147.1180, Valid Loss: 156.2113\n",
      "Epoch: 30, Train Loss: 146.2793, Valid Loss: 154.2727\n",
      "Epoch: 40, Train Loss: 145.4611, Valid Loss: 155.9730\n",
      "Epoch: 50, Train Loss: 145.1783, Valid Loss: 155.0597\n",
      "Epoch: 60, Train Loss: 145.3470, Valid Loss: 156.2179\n",
      "Epoch: 70, Train Loss: 145.2746, Valid Loss: 155.6121\n",
      "Epoch: 80, Train Loss: 145.2495, Valid Loss: 155.5777\n",
      "Epoch: 90, Train Loss: 145.1883, Valid Loss: 155.4424\n",
      "Epoch: 100, Train Loss: 145.1555, Valid Loss: 155.3582\n",
      "Epoch: 110, Train Loss: 145.2264, Valid Loss: 155.7528\n",
      "Epoch: 120, Train Loss: 145.0911, Valid Loss: 155.3884\n",
      "Epoch: 130, Train Loss: 145.0830, Valid Loss: 155.2634\n",
      "Epoch: 140, Train Loss: 145.2202, Valid Loss: 155.9046\n",
      "Epoch: 150, Train Loss: 145.0477, Valid Loss: 155.5220\n",
      "Epoch: 160, Train Loss: 144.9859, Valid Loss: 155.2039\n",
      "Epoch: 170, Train Loss: 145.0800, Valid Loss: 155.5700\n",
      "Epoch: 180, Train Loss: 145.0245, Valid Loss: 155.6092\n",
      "Epoch: 190, Train Loss: 144.9211, Valid Loss: 155.3138\n",
      "Epoch: 200, Train Loss: 144.8818, Valid Loss: 155.2016\n",
      "Epoch: 210, Train Loss: 144.9418, Valid Loss: 155.4044\n",
      "Epoch: 220, Train Loss: 144.8686, Valid Loss: 155.3736\n",
      "Epoch: 230, Train Loss: 144.7949, Valid Loss: 155.2732\n",
      "Epoch: 240, Train Loss: 144.8070, Valid Loss: 155.2319\n",
      "Epoch: 250, Train Loss: 144.9344, Valid Loss: 155.1022\n",
      "Epoch: 260, Train Loss: 144.8594, Valid Loss: 154.9055\n",
      "Epoch: 270, Train Loss: 144.8603, Valid Loss: 154.8832\n",
      "Epoch: 280, Train Loss: 144.7213, Valid Loss: 154.8905\n",
      "Epoch: 290, Train Loss: 144.7386, Valid Loss: 155.1726\n",
      "Epoch: 300, Train Loss: 144.8773, Valid Loss: 155.1326\n",
      "Epoch: 310, Train Loss: 144.5236, Valid Loss: 154.5219\n",
      "Epoch: 320, Train Loss: 145.3032, Valid Loss: 155.9877\n",
      "Epoch: 330, Train Loss: 144.5998, Valid Loss: 155.3038\n",
      "Epoch: 340, Train Loss: 144.5609, Valid Loss: 155.4004\n",
      "Epoch: 350, Train Loss: 144.5771, Valid Loss: 155.5512\n",
      "Epoch: 360, Train Loss: 144.3885, Valid Loss: 154.9898\n",
      "Epoch: 370, Train Loss: 145.2295, Valid Loss: 155.6419\n",
      "Epoch: 380, Train Loss: 144.6053, Valid Loss: 155.6501\n",
      "Epoch: 390, Train Loss: 144.3943, Valid Loss: 155.2883\n",
      "Epoch: 400, Train Loss: 144.7007, Valid Loss: 154.9960\n",
      "Epoch: 410, Train Loss: 144.4802, Valid Loss: 154.9978\n",
      "Epoch: 420, Train Loss: 144.6407, Valid Loss: 155.1803\n",
      "Epoch: 430, Train Loss: 144.6037, Valid Loss: 155.0627\n",
      "Epoch: 440, Train Loss: 144.3337, Valid Loss: 155.4196\n",
      "Epoch: 450, Train Loss: 144.3151, Valid Loss: 155.0894\n",
      "Epoch: 460, Train Loss: 144.2774, Valid Loss: 155.2687\n",
      "Epoch: 470, Train Loss: 144.2533, Valid Loss: 155.1156\n",
      "Epoch: 480, Train Loss: 144.3169, Valid Loss: 155.0571\n",
      "Epoch: 490, Train Loss: 144.2542, Valid Loss: 155.0785\n",
      "Epoch: 500, Train Loss: 144.2022, Valid Loss: 155.3619\n",
      "Epoch: 510, Train Loss: 144.0812, Valid Loss: 154.2040\n",
      "Epoch: 520, Train Loss: 144.1112, Valid Loss: 154.2436\n",
      "Epoch: 530, Train Loss: 144.1112, Valid Loss: 154.2490\n",
      "Epoch: 540, Train Loss: 144.1588, Valid Loss: 154.0795\n",
      "Epoch: 550, Train Loss: 144.1305, Valid Loss: 154.3910\n",
      "Epoch: 560, Train Loss: 143.9171, Valid Loss: 154.2056\n",
      "Epoch: 570, Train Loss: 144.2306, Valid Loss: 154.0180\n",
      "Epoch: 580, Train Loss: 144.2713, Valid Loss: 153.6307\n",
      "Epoch: 590, Train Loss: 144.3188, Valid Loss: 153.9320\n",
      "Epoch: 600, Train Loss: 144.1444, Valid Loss: 154.0551\n",
      "Epoch: 610, Train Loss: 144.1168, Valid Loss: 154.1174\n",
      "Epoch: 620, Train Loss: 144.2013, Valid Loss: 156.0205\n",
      "Epoch: 630, Train Loss: 143.9083, Valid Loss: 153.7768\n",
      "Epoch: 640, Train Loss: 144.2857, Valid Loss: 153.8158\n",
      "Epoch: 650, Train Loss: 144.0623, Valid Loss: 155.6198\n",
      "Epoch: 660, Train Loss: 144.1811, Valid Loss: 154.0988\n",
      "Epoch: 670, Train Loss: 144.5377, Valid Loss: 154.8496\n",
      "Epoch: 680, Train Loss: 144.0236, Valid Loss: 154.7382\n",
      "Epoch: 690, Train Loss: 143.8512, Valid Loss: 154.1953\n",
      "Epoch: 700, Train Loss: 144.4066, Valid Loss: 154.5885\n",
      "Epoch: 710, Train Loss: 144.8860, Valid Loss: 154.5639\n",
      "Epoch: 720, Train Loss: 144.4390, Valid Loss: 155.5265\n",
      "Epoch: 730, Train Loss: 143.8689, Valid Loss: 153.9467\n",
      "Epoch: 740, Train Loss: 144.0614, Valid Loss: 155.6597\n",
      "Epoch: 750, Train Loss: 144.3550, Valid Loss: 154.8230\n",
      "Epoch: 760, Train Loss: 144.0016, Valid Loss: 154.8572\n",
      "Epoch: 770, Train Loss: 143.9016, Valid Loss: 155.0240\n",
      "Epoch: 780, Train Loss: 143.9636, Valid Loss: 155.2394\n",
      "Epoch: 790, Train Loss: 143.8106, Valid Loss: 154.3474\n",
      "Epoch: 800, Train Loss: 143.6375, Valid Loss: 153.8048\n",
      "Epoch: 810, Train Loss: 143.6744, Valid Loss: 154.3973\n",
      "Epoch: 820, Train Loss: 143.6348, Valid Loss: 155.0004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\Documents\\GitHub\\EmotionAnalysis\\notebooks\\Methods 1 and 2\\learnt_mappings.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marti/Documents/GitHub/EmotionAnalysis/notebooks/Methods%201%20and%202/learnt_mappings.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, train_loader, dev_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\marti\\Documents\\GitHub\\EmotionAnalysis\\notebooks\\Methods 1 and 2\\functions.py:50\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     49\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mfor\u001b[39;00m i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     51\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     52\u001b[0m         y_pred \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\BDLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\BDLenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\BDLenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\BDLenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\marti\\Documents\\GitHub\\EmotionAnalysis\\notebooks\\Methods 1 and 2\\functions.py:37\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     36\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miloc[index]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m---> 37\u001b[0m     row \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(row)\u001b[39m.\u001b[39;49mfloat()\n\u001b[0;32m     38\u001b[0m     features \u001b[39m=\u001b[39m row[:\u001b[39m7\u001b[39m]\n\u001b[0;32m     39\u001b[0m     labels \u001b[39m=\u001b[39m row[\u001b[39m7\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, dev_loader, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0516a4f0134d11d727d4d6ded12c3bd56d2d21755734cb8813876ac5444aae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
